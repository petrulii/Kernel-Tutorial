{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "369865ab-aea6-4e5c-af8f-4a2d360de4d6",
   "metadata": {},
   "source": [
    "## Introduction to Kernel Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f319e0d5-d270-4a5d-915a-3710f2f26dec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dbdf4790",
   "metadata": {},
   "source": [
    "We will look at some first examples with synthetic data. We'll compare how different classical and kernel methods perform when trying to find the best label predictions for synthetic data points. The synthetic data points $x_1, x_2, \\ldots, x_n$ are obtained by sampling random values from the uniform distribution. Their labels $y_1, y_2, \\ldots, y_n$ are generated using a non-linear target function defined as:\n",
    "\n",
    "$ y_i = f(x_i) = 10 \\left( \\sin(3(1 - x_i)) \\cdot \\tanh(5(1 - x_i)) + \\left(\\frac{x_i - 0.3}{0.9}\\right)^2 - 0.514 \\right) $\n",
    "\n",
    "In the code, we have $X = x_1, x_2, \\ldots, x_n$ and $y = y_1, y_2, \\ldots, y_n$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b99c152-39d8-4078-bca1-53e074906e7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def nonlinear_target_function(x):\n",
    "    \"\"\"\n",
    "    param `x` (data points for which the target y is generated)\n",
    "    \"\"\"\n",
    "    return 10 * (np.sin(3 * (1 - x)) * np.tanh(5 * (1 - x)) + ((x-0.3)/0.9)**2 - 0.514)\n",
    "\n",
    "def generate_data(n, sigma=2, low = -1, high = 1):\n",
    "    \"\"\"\n",
    "    param `n`: number of data points to generate\n",
    "    param `sigma`: standard deviation of noise for the uniform distribution\n",
    "    param `low`: lower bound of data points\n",
    "    param `high`: upper bound of data points\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng()\n",
    "    X = rng.uniform(low=low, high=high, size=(n,))\n",
    "    \n",
    "    targets = nonlinear_target_function(X)\n",
    "    y = targets + sigma * rng.normal(size=(n,))\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# Here we generate 100 data points X and their corresponding labels y\n",
    "n = 100\n",
    "X, y = generate_data(n)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fda9b077",
   "metadata": {},
   "source": [
    "Here we visualize both the target function and the generated data. The target function is plotted using a continuous line, while the data points are scattered in red. The **target function** illustrates the true pattern that we hope to find using different methods, and the **data points** show the noisy observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284cf41e-3623-4b7f-a305-ac353303bc0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "low, high = -1, 1\n",
    "xs = np.linspace(low, high, 100)\n",
    "plt.plot(xs, nonlinear_target_function(xs), label=\"Target function\")\n",
    "plt.scatter(X, y, label=\"Data\", marker='.', color=\"red\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "62a7d2ad",
   "metadata": {},
   "source": [
    "The *eval_solution* function evaluates the performance of a predictor function and visualizes its predictions compared to the true target function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc23d6b-2295-464e-bf07-c4350377ef84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def eval_solution(predictor, X, y):\n",
    "    \"\"\"\n",
    "    param `predictor`: a function that takes a single input and returns a prediction\n",
    "    param `X`: input data points used for training\n",
    "    param `y`: true labels corresponding to the training data\n",
    "    \"\"\"\n",
    "    predictions_train = np.array([predictor(x) for x in X])\n",
    "    mse_train = np.mean((y - predictions_train)**2)\n",
    "    print(f\"MSE on training data : {mse_train:.2f}\")\n",
    "    \n",
    "    xs = np.linspace(low, high, 500)\n",
    "    predictions = np.array([predictor(x) for x in xs])\n",
    "    targets = nonlinear_target_function(xs)\n",
    "    mse = np.mean((targets - predictions)**2)\n",
    "    print(f\"MSE to target function: {mse:.2f}\")\n",
    "\n",
    "    plt.plot(xs, targets, label=\"Target\")\n",
    "    plt.scatter(X, y, label=\"Data\", marker='.', color=\"red\")\n",
    "    plt.plot(xs, predictions, label=\"Predictions\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c7997c4b-89ee-4c81-a161-1d40ec425d41",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "777c3de6",
   "metadata": {},
   "source": [
    "Here we use a classical linear model: *Linear Regression* to find a linear prediction function for the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6471c9-8d56-4bee-bba8-ec3b72a91ae5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Fit a linear regression model\n",
    "##### Fill here ! #####\n",
    "\n",
    "# Obtain the coefficients\n",
    "alpha = ##### Fill here ! #####\n",
    "\n",
    "# Create the linear predictor function\n",
    "predictor = lambda x: alpha * x\n",
    "\n",
    "# Evaluate the solution\n",
    "eval_solution(predictor, X, y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1a919b94-8c48-4462-88eb-8034072ae458",
   "metadata": {},
   "source": [
    "### Kernel Regression on Vectors"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0c2964e3",
   "metadata": {},
   "source": [
    "In machine learning, a kernel is a function that computes the similarity (or inner product) between two data points in a higher-dimensional space. Mathematically, a kernel $k$ between two data points $x$ and $x'$ is defined as $k(x, x') = \\langle\\psi(x), \\psi(x')\\rangle_X$, where $\\psi$ is a function that maps the data points into a higher-dimensional space. Here we try to find different prediction functions for our synthetic data using different kernels such as the linear kernel, the polynomial kernel, and the Gaussian (RBF) kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d8e1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_kernel_matrix(kernel, X):\n",
    "    \"\"\"\n",
    "    Compute the kernel matrix for a given kernel function and input data.\n",
    "\n",
    "    Parameters:\n",
    "    - kernel: The kernel function.\n",
    "    - X: Input data.\n",
    "\n",
    "    Returns:\n",
    "    - K: The computed kernel matrix.\n",
    "    \"\"\"\n",
    "    n = len(X)\n",
    "    K = np.empty((n, n))\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            # Calculate the kernel value for each pair of data points.\n",
    "            ##### Fill here ! #####\n",
    "    return K\n",
    "\n",
    "def kernel_ridge(K, y, lbd=0.1):\n",
    "    \"\"\"\n",
    "    Perform kernel ridge regression.\n",
    "\n",
    "    Parameters:\n",
    "    - K: The kernel matrix.\n",
    "    - y: Target values.\n",
    "    - lbd: Regularization parameter.\n",
    "\n",
    "    Returns:\n",
    "    - alpha: Coefficients obtained from kernel ridge regression.\n",
    "    \"\"\"\n",
    "    n = len(K)\n",
    "    # Solve the linear system using the kernel matrix, target values, and regularization parameter.\n",
    "    alpha, _, _, _ = np.linalg.lstsq(K + lbd * np.identity(n), y, rcond=None)\n",
    "    return alpha\n",
    "\n",
    "def make_predictor(kernel, X, alpha):\n",
    "    \"\"\"\n",
    "    Create a predictor function based on the kernel, input data, and obtained coefficients.\n",
    "\n",
    "    Parameters:\n",
    "    - kernel: The kernel function.\n",
    "    - X: Input data.\n",
    "    - alpha: Coefficients obtained from kernel ridge regression.\n",
    "\n",
    "    Returns:\n",
    "    - predictor: The predictor function.\n",
    "    \"\"\"\n",
    "    n = len(X)\n",
    "    def predictor(x):\n",
    "        # Use the obtained coefficients to make predictions for a new data point.\n",
    "        prediction = 0\n",
    "        for i in range(n):\n",
    "            ##### Fill here ! #####\n",
    "        return prediction\n",
    "    return predictor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a269064-45bb-4311-bb9c-facfc4f9cdc8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a linear kernel function\n",
    "def linear_kernel(x, xp):\n",
    "    \"\"\"\n",
    "    Linear kernel function.\n",
    "\n",
    "    Parameters:\n",
    "    - x: First data point.\n",
    "    - xp: Second data point.\n",
    "\n",
    "    Returns:\n",
    "    - Result of the linear kernel between x and xp.\n",
    "    \"\"\"\n",
    "    return ##### Fill here ! #####\n",
    "\n",
    "# Compute the kernel matrix using the linear kernel\n",
    "K = compute_kernel_matrix(linear_kernel, X)\n",
    "\n",
    "# Perform kernel ridge regression to obtain coefficients\n",
    "alpha = kernel_ridge(K, y)\n",
    "\n",
    "# Create a predictor function based on the linear kernel and obtained coefficients\n",
    "predictor = make_predictor(linear_kernel, X, alpha)\n",
    "\n",
    "# Evaluate the performance of the linear predictor\n",
    "eval_solution(predictor, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c1af61-bb24-48ac-aef0-c472396c7fdb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a second-degree polynomial kernel function\n",
    "def polynomial_kernel_2(x, xp):\n",
    "    \"\"\"\n",
    "    Second-degree polynomial kernel function.\n",
    "\n",
    "    Parameters:\n",
    "    - x: First data point.\n",
    "    - xp: Second data point.\n",
    "\n",
    "    Returns:\n",
    "    - Result of the polynomial kernel of degree 2 between x and xp.\n",
    "    \"\"\"\n",
    "    return ##### Fill here ! (look at the expression for the 3d order pol. kernel below) #####\n",
    "\n",
    "# Compute the kernel matrix using the second-degree polynomial kernel\n",
    "K = compute_kernel_matrix(polynomial_kernel_2, X)\n",
    "\n",
    "# Perform kernel ridge regression to obtain coefficients\n",
    "alpha = kernel_ridge(K, y)\n",
    "\n",
    "# Create a predictor function based on the second-degree polynomial kernel and obtained coefficients\n",
    "predictor = make_predictor(polynomial_kernel_2, X, alpha)\n",
    "\n",
    "# Evaluate the performance of the polynomial predictor\n",
    "eval_solution(predictor, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e509407-7b08-46eb-8ae1-ce7e5db39892",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a third-degree polynomial kernel function\n",
    "def polynomial_kernel_3(x, xp):\n",
    "    \"\"\"\n",
    "    Third-degree polynomial kernel function.\n",
    "\n",
    "    Parameters:\n",
    "    - x: First data point.\n",
    "    - xp: Second data point.\n",
    "\n",
    "    Returns:\n",
    "    - Result of the polynomial kernel of degree 3 between x and xp.\n",
    "    \"\"\"\n",
    "    return x * xp + x**2 * xp**2 + x**3 * xp**3\n",
    "\n",
    "# Compute the kernel matrix using the third-degree polynomial kernel\n",
    "K = compute_kernel_matrix(polynomial_kernel_3, X)\n",
    "\n",
    "# Perform kernel ridge regression to obtain coefficients\n",
    "alpha = kernel_ridge(K, y)\n",
    "\n",
    "# Create a predictor function based on the third-degree polynomial kernel and obtained coefficients\n",
    "predictor = make_predictor(polynomial_kernel_3, X, alpha)\n",
    "\n",
    "# Evaluate the performance of the polynomial predictor\n",
    "eval_solution(predictor, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2264d7-c6dc-4386-bbcd-89d68dc3d516",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the bandwidth for the Gaussian kernel\n",
    "bw = 0.5\n",
    "\n",
    "# Define a Gaussian kernel function\n",
    "def gaussian_kernel(x, xp):\n",
    "    \"\"\"\n",
    "    Gaussian kernel function.\n",
    "\n",
    "    Parameters:\n",
    "    - x: First data point.\n",
    "    - xp: Second data point.\n",
    "\n",
    "    Returns:\n",
    "    - Result of the Gaussian kernel between x and xp.\n",
    "    \"\"\"\n",
    "    return np.exp(-(x - xp)**2 / (2 * bw**2))\n",
    "\n",
    "# Compute the kernel matrix using the Gaussian kernel\n",
    "K = compute_kernel_matrix(gaussian_kernel, X)\n",
    "\n",
    "# Perform kernel ridge regression to obtain coefficients\n",
    "alpha = kernel_ridge(K, y)\n",
    "\n",
    "# Create a predictor function based on the Gaussian kernel and obtained coefficients\n",
    "predictor = make_predictor(gaussian_kernel, X, alpha)\n",
    "\n",
    "# Evaluate the performance of the Gaussian predictor\n",
    "eval_solution(predictor, X, y)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "007ae579",
   "metadata": {},
   "source": [
    "A Gaussian kernel with a too small bandwidth *bw* can lead to overfitting in kernel methods. Overfitting occurs when a model learns to perform well on the training data but fails to generalize to new, unseen data. A small bandwidth in the Gaussian kernel results in a narrow Gaussian function. This means that the kernel assigns significant weight only to points very close to each data point. As a result, the model becomes highly sensitive to the specificities of the training data, capturing noise and outliers rather than the true underlying pattern (target function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae09650-23a6-46e1-9af3-04b5a76a4c1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the bandwidth for the Gaussian kernel\n",
    "bw = 0.01\n",
    "\n",
    "# Compute the kernel matrix using the Gaussian kernel\n",
    "K = compute_kernel_matrix(gaussian_kernel, X)\n",
    "\n",
    "# Perform kernel ridge regression to obtain coefficients\n",
    "alpha = kernel_ridge(K, y)\n",
    "\n",
    "# Create a predictor function based on the Gaussian kernel and obtained coefficients\n",
    "predictor = make_predictor(gaussian_kernel, X, alpha)\n",
    "\n",
    "# Evaluate the performance of the Gaussian predictor\n",
    "eval_solution(predictor, X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e06db4-7abb-4014-8f89-0d16101e3f9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a minimum kernel function\n",
    "def min_kernel(x, xp):\n",
    "    \"\"\"\n",
    "    Minimum kernel function.\n",
    "\n",
    "    Parameters:\n",
    "    - x: First data point.\n",
    "    - xp: Second data point.\n",
    "\n",
    "    Returns:\n",
    "    - Minimum value between x and xp.\n",
    "    \"\"\"\n",
    "    return ##### Fill here ! #####\n",
    "\n",
    "# Compute the kernel matrix using the minimum kernel\n",
    "K = compute_kernel_matrix(min_kernel, X)\n",
    "\n",
    "# Perform kernel ridge regression to obtain coefficients\n",
    "alpha = kernel_ridge(K, y)\n",
    "\n",
    "# Create a predictor function based on the minimum kernel and obtained coefficients\n",
    "predictor = make_predictor(min_kernel, X, alpha)\n",
    "\n",
    "# Evaluate the performance of the minimum kernel predictor\n",
    "eval_solution(predictor, X, y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ccb0f263",
   "metadata": {},
   "source": [
    "The sum kernel is formed by summing the minimum and Gaussian kernels as follows:\n",
    "\n",
    "$K_{\\text{sum}}(x, xp) = K_{\\text{min}}(x, xp) + K_{\\text{RBF}}(x, xp)$\n",
    "\n",
    "The sum kernel incorporates both the local agreement captured by the minimum kernel and the smooth, global relationships captured by the Gaussian kernel. This combination allows the sum kernel to balance the importance of local and global structures in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60be725-20e5-4215-bab1-15321cc04dd7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the bandwidth for the Gaussian kernel\n",
    "bw = 0.5\n",
    "\n",
    "# Define a sum kernel by combining the minimum and Gaussian kernels\n",
    "def sum_kernel(x, xp):\n",
    "    \"\"\"\n",
    "    Sum kernel function.\n",
    "\n",
    "    Parameters:\n",
    "    - x: First data point.\n",
    "    - xp: Second data point.\n",
    "\n",
    "    Returns:\n",
    "    - Sum of the minimum and Gaussian kernels between x and xp.\n",
    "    \"\"\"\n",
    "    return ##### Fill here ! #####\n",
    "\n",
    "# Compute the kernel matrix using the sum kernel\n",
    "K = compute_kernel_matrix(sum_kernel, X)\n",
    "\n",
    "# Perform kernel ridge regression to obtain coefficients\n",
    "alpha = kernel_ridge(K, y)\n",
    "\n",
    "# Create a predictor function based on the sum kernel and obtained coefficients\n",
    "predictor = make_predictor(sum_kernel, X, alpha)\n",
    "\n",
    "# Evaluate the performance of the sum kernel predictor\n",
    "eval_solution(predictor, X, y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3a7dcd21-9b9b-4b3c-8b24-a444ad7f1d35",
   "metadata": {},
   "source": [
    "## Kernel Regression on Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43ce1e2-1eeb-498a-b06d-d1b25ae75829",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from gklearn.utils.graph_files import load_dataset\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "57673677-2f0e-450f-a397-8e9483526104",
   "metadata": {},
   "source": [
    "When applied to graphs, kernel methods allow us to exploit the structural information encoded in graph data. In this context, each graph is treated as a data point, and kernels measure the similarity between pairs of graphs.\n",
    "\n",
    "*The following code is adapted from a practical by https://scholar.google.com/citations?user=YqmqE9gAAAAJ&hl=en. The dataset a modified version of https://brunl01.users.greyc.fr/CHEMISTRY/index.html#Acyclic.*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "be0075ab",
   "metadata": {},
   "source": [
    "We first load a graph dataset (`acyclic/dataset_bps.ds`) along with associated target values. The dataset consists of graphs (molecules) and corresponding boiling temperature values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0079075-436b-48ed-957e-6c7dba04ddf8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "G, y, info = load_dataset(\"acyclic/dataset_bps.ds\")\n",
    "y = np.array(y)\n",
    "N = len(G)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "72646ae6",
   "metadata": {},
   "source": [
    "Some exploratory analysis is performed on the dataset, such as visualizing two example graphs, computing the mean and variance of the target temperatures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62656643-abef-4797-9519-06c6586ea31b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visualizing Two Graphs Side by Side\n",
    "\n",
    "# Plotting the first graph (G1)\n",
    "x1 = 92\n",
    "G1 = G[x1]\n",
    "print(\"Label for G1\", y[x1])\n",
    "label = \"atom_symbol\"\n",
    "print(\"Atoms of G1\", nx.get_node_attributes(G1 ,label))\n",
    "plt.subplot(1, 2, 1)  # Subplot for the first graph\n",
    "nx.draw_networkx(G1)\n",
    "plt.title(f'Graph {x1} (Label: {y[x1]:.2f})')\n",
    "\n",
    "# Plotting the second graph (G2)\n",
    "x2 = 90\n",
    "G2 = G[x2]\n",
    "print(\"Label for G2\", y[x2])\n",
    "print(\"Atoms of G2\", nx.get_node_attributes(G2 ,label))\n",
    "plt.subplot(1, 2, 2)  # Subplot for the second graph\n",
    "nx.draw_networkx(G2)\n",
    "plt.title(f'Graph {x2} (Label: {y[x2]:.2f})')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Displaying Mean and Variance of Temperatures\n",
    "print(\"Mean temperature\", np.mean(y))\n",
    "print(\"Variance of temperature\", np.mean((y - np.mean(y))**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b11e661-fad7-42a7-956a-8a44c68e3da3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "import numpy as np\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "def train_and_evaluate_graph_kernel_method(K):\n",
    "\n",
    "    # Visualize the computed kernel matrix as an image plot\n",
    "    plt.imshow(K)\n",
    "\n",
    "    # Compute and print the minimum eigenvalue of the kernel matrix\n",
    "    print(\"Min eigenvalue: \", np.min(np.linalg.eigvalsh(K)))\n",
    "\n",
    "    # Initialize Support Vector Regression (SVR) with a precomputed kernel\n",
    "    clf = SVR(kernel=\"precomputed\")\n",
    "\n",
    "    # Define a random splitting of the dataset\n",
    "    rs = ShuffleSplit(n_splits=5, test_size=0.33, random_state=0)\n",
    "\n",
    "    # Initialize an empty list to store errors\n",
    "    errors = []\n",
    "\n",
    "    # Create an array representing the dataset indices\n",
    "    dataset = np.arange(len(G))\n",
    "\n",
    "    # Iterate through train and validation sets in each split\n",
    "    for train_index, val_index in rs.split(dataset):\n",
    "        # Extract training and validation kernel matrices\n",
    "        Ktrain = K[train_index, :][:, train_index]\n",
    "        Kval = K[val_index, :][:, train_index]\n",
    "        \n",
    "        # Fit SVR model on training data\n",
    "        clf.fit(Ktrain, y[train_index])\n",
    "        \n",
    "        # Predict on the validation set\n",
    "        y_pred = clf.predict(Kval)\n",
    "        \n",
    "        # Calculate and store absolute errors\n",
    "        errors.extend(np.abs(y_pred - y[val_index]))\n",
    "\n",
    "    # Print the Mean Squared Error (MSE) over all splits\n",
    "    print(\"MSE:\", np.mean(errors))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "644d827d",
   "metadata": {},
   "source": [
    "To first apply kernel methods for vectors, we convert the categorical node attribute \"atom_symbol\" into a numerical format using a simple feature map (fingerprint). This transformation results in a binary vector representation for each graph, capturing the presence or absence of different atom symbols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0af55d-fe89-429f-8010-6bbdef932464",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fingerprint(G):\n",
    "    \"\"\"\n",
    "    Generate a fingerprint matrix for a list of graphs based on node attributes.\n",
    "\n",
    "    Parameters:\n",
    "    - G: List of graphs.\n",
    "\n",
    "    Returns:\n",
    "    - Fingerprint matrix with one-hot encodings for node attributes.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get unique atom symbols from all graphs and sort them\n",
    "    unique_labels = list(set(l for Gi in G for l in nx.get_node_attributes(Gi, \"atom_symbol\").values()))\n",
    "    unique_labels.sort()\n",
    "\n",
    "    # Create an empty numpy array to store the fingerprint encodings\n",
    "    num_dicts = len(G)\n",
    "    num_labels = len(unique_labels)\n",
    "    encoded_array = np.zeros((num_dicts, num_labels), dtype=int)\n",
    "\n",
    "    # Map labels to their index in unique_labels\n",
    "    label_to_index = {label: i for i, label in enumerate(unique_labels)}\n",
    "\n",
    "    # Populate the numpy array with encodings for each dictionary\n",
    "    for i, Gi in enumerate(G):\n",
    "        for key, label in nx.get_node_attributes(Gi, \"atom_symbol\").items():\n",
    "            index = label_to_index[label]\n",
    "            encoded_array[i, index] = 1\n",
    "\n",
    "    return encoded_array"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f714a4db",
   "metadata": {},
   "source": [
    "We use a radial basis function (RBF) kernel to measure the similarity between graphs. The RBF kernel captures the structural similarity between graphs based on fingerprint representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d5e877-61cc-455c-9c42-a7789e566592",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate fingerprint features for all graphs\n",
    "features = fingerprint(G)\n",
    "print(f'Graph {x1} (Fingerprint: {features[x1]})')\n",
    "print(f'Graph {x2} (Fingerprint: {features[x2]})')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "165515ae",
   "metadata": {},
   "source": [
    "Now we use the Support Vector Regression (SVR) model with an RBF kernel on the graph fingerprints to predict target values. SVR is a machine learning algorithm that finds a hyperplane in a high-dimensional space to minimize the regression error. The kernel matrix computed from the RBF kernel is used as input to SVR, allowing the algorithm to operate directly on the graph similarities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bdf93ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rbf_kernel(x, y, sigma=1):\n",
    "    \"\"\"\n",
    "    Radial Basis Function (RBF) kernel function.\n",
    "\n",
    "    Parameters:\n",
    "    - x: First data point.\n",
    "    - y: Second data point.\n",
    "    - sigma: RBF kernel parameter (default is 1).\n",
    "\n",
    "    Returns:\n",
    "    - Result of the RBF kernel between x and y.\n",
    "    \"\"\"\n",
    "    return np.exp(- (np.linalg.norm(x - y) ** 2) / (2 * sigma ** 2))\n",
    "\n",
    "# Initialize an empty kernel matrix\n",
    "n = len(features)\n",
    "K_fp = np.zeros((n, n))\n",
    "\n",
    "# Compute the kernel matrix using the RBF kernel\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        K_fp[i, j] = rbf_kernel(features[i], features[j])\n",
    "\n",
    "train_and_evaluate_graph_kernel_method(K_fp)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b937f0e3",
   "metadata": {},
   "source": [
    "Now we use the treelet kernel, a krenel that works **directly on the graphs** rather than their vector representations. Treelets are graph fragments that capture local substructures. The treelet kernel measures the similarity between graphs based on the number of common treelets, providing a more interpretable notion of similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d8e2e7-7bf7-4245-afed-f253b2e40eb2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from gklearn.kernels import treeletKernel\n",
    "\n",
    "# Compute the treelet kernel matrix for the entire dataset of graphs\n",
    "K_tree, run_time = treeletKernel.treeletkernel(##### Fill here ! #####)\n",
    "\n",
    "train_and_evaluate_graph_kernel_method(K_tree)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "98b278ef",
   "metadata": {},
   "source": [
    "Since the Treelet kernel does not perform better than the regressing on fingerprints, we use another kernel that works **directly on the graphs**. The Weisfeiler-Lehman (WL) kernel was introduced by Benjamin Weisfeiler and Alexander Lehman in 1968, it works by running a labelling-aggregation algorithm on both graphs and comparing the two algorithm runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774317a5-a87b-4d00-89e3-048c2ddb52e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from gklearn.kernels import weisfeilerLehmanKernel\n",
    "\n",
    "# Compute the Weisfeiler Lehman kernel matrix for the entire dataset of graphs\n",
    "K_WL, run_time = weisfeilerLehmanKernel.weisfeilerlehmankernel(##### Fill here ! #####)\n",
    "\n",
    "train_and_evaluate_graph_kernel_method(K_WL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25735ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pair-wise sum of positive definite kernels is a positive definite kernel\n",
    "K_sum = ##### Fill here ! #####\n",
    "\n",
    "train_and_evaluate_graph_kernel_method(K_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ca2a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pair-wise product of positive definite kernels is a positive definite kernel\n",
    "K_prod = ##### Fill here ! #####\n",
    "\n",
    "train_and_evaluate_graph_kernel_method(K_prod)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
